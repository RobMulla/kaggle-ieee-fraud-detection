{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering 001\n",
    "\n",
    "https://www.kaggle.com/robikscube/ieee-gb-2-make-amount-useful-again/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet('../../input/train.parquet')\n",
    "test_df = pd.read_parquet('../../input/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape control: (590540, 433) (506691, 432)\n"
     ]
    }
   ],
   "source": [
    "print('Shape control:', train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Features - Month - Week - Dayofway - DayofWeek, Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "for df in [train_df, test_df]:\n",
    "    # Temporary\n",
    "    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "    df['DT_M'] = (df['DT'].dt.year-2017)*12 + df['DT'].dt.month\n",
    "    df['DT_W'] = (df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear\n",
    "    df['DT_D'] = (df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear\n",
    "    \n",
    "    df['DT_hour'] = df['DT'].dt.hour\n",
    "    df['DT_day_week'] = df['DT'].dt.dayofweek\n",
    "    df['DT_day'] = df['DT'].dt.day\n",
    "    \n",
    "    # D9 column\n",
    "    df['D9_fixed'] = np.where(df['D9'].isna(),0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill in Noise Card1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_cols = ['card1']\n",
    "\n",
    "for col in i_cols: \n",
    "    valid_card = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    valid_card = valid_card[col].value_counts()\n",
    "    valid_card = valid_card[valid_card>2]\n",
    "    valid_card = list(valid_card.index)\n",
    "\n",
    "    train_df[col] = np.where(train_df[col].isin(test_df[col]), train_df[col], np.nan)\n",
    "    test_df[col]  = np.where(test_df[col].isin(train_df[col]), test_df[col], np.nan)\n",
    "\n",
    "    train_df[col] = np.where(train_df[col].isin(valid_card), train_df[col], np.nan)\n",
    "    test_df[col]  = np.where(test_df[col].isin(valid_card), test_df[col], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M sum and M na count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_cols = ['M1','M2','M3','M5','M6','M7','M8','M9']\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    df['M_sum'] = df[i_cols].sum(axis=1).astype(np.int8)\n",
    "    df['M_na'] = df[i_cols].isna().sum(axis=1).astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Mean for ProductID and M4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'isFraud'\n",
    "\n",
    "for col in ['ProductCD','M4']:\n",
    "    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n",
    "                                                        columns={'mean': col+'_target_mean'})\n",
    "    temp_dict.index = temp_dict[col].values\n",
    "    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n",
    "\n",
    "    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n",
    "    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transaction AMT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### TransactionAmt\n",
    "\n",
    "# Let's add some kind of client uID based on cardID ad addr columns\n",
    "# The value will be very specific for each client so we need to remove it\n",
    "# from final feature. But we can use it for aggregations.\n",
    "train_df['uid'] = train_df['card1'].astype(str)+'_'+train_df['card2'].astype(str)\n",
    "test_df['uid'] = test_df['card1'].astype(str)+'_'+test_df['card2'].astype(str)\n",
    "\n",
    "train_df['uid2'] = train_df['uid'].astype(str)+'_'+train_df['card3'].astype(str)+'_'+train_df['card5'].astype(str)\n",
    "test_df['uid2'] = test_df['uid'].astype(str)+'_'+test_df['card3'].astype(str)+'_'+test_df['card5'].astype(str)\n",
    "\n",
    "train_df['uid3'] = train_df['uid2'].astype(str)+'_'+train_df['addr1'].astype(str)+'_'+train_df['addr2'].astype(str)\n",
    "test_df['uid3'] = test_df['uid2'].astype(str)+'_'+test_df['addr1'].astype(str)+'_'+test_df['addr2'].astype(str)\n",
    "\n",
    "# Check if the Transaction Amount is common or not (we can use freq encoding here)\n",
    "# In our dialog with a model we are telling to trust or not to these values   \n",
    "train_df['TransactionAmt_check'] = np.where(train_df['TransactionAmt'].isin(test_df['TransactionAmt']), 1, 0)\n",
    "test_df['TransactionAmt_check']  = np.where(test_df['TransactionAmt'].isin(train_df['TransactionAmt']), 1, 0)\n",
    "\n",
    "# For our model current TransactionAmt is a noise\n",
    "# https://www.kaggle.com/kyakovlev/ieee-check-noise\n",
    "# (even if features importances are telling contrariwise)\n",
    "# There are many unique values and model doesn't generalize well\n",
    "# Lets do some aggregations\n",
    "i_cols = ['card1','card2','card3','card5','uid','uid2','uid3']\n",
    "\n",
    "for col in i_cols:\n",
    "    for agg_type in ['mean','std']:\n",
    "        new_col_name = col+'_TransactionAmt_'+agg_type\n",
    "        temp_df = pd.concat([train_df[[col, 'TransactionAmt']], test_df[[col,'TransactionAmt']]])\n",
    "        #temp_df['TransactionAmt'] = temp_df['TransactionAmt'].astype(int)\n",
    "        temp_df = temp_df.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n",
    "                                                columns={agg_type: new_col_name})\n",
    "        \n",
    "        temp_df.index = list(temp_df[col])\n",
    "        temp_df = temp_df[new_col_name].to_dict()   \n",
    "    \n",
    "        train_df[new_col_name] = train_df[col].map(temp_df)\n",
    "        test_df[new_col_name]  = test_df[col].map(temp_df)\n",
    "           \n",
    "# Small \"hack\" to transform distribution \n",
    "# (doesn't affect auc much, but I like it more)\n",
    "# please see how distribution transformation can boost your score \n",
    "# (not our case but related)\n",
    "# https://scikit-learn.org/stable/auto_examples/compose/plot_transformed_target.html\n",
    "train_df['TransactionAmt_log1p'] = np.log1p(train_df['TransactionAmt'])\n",
    "test_df['TransactionAmt_log1p'] = np.log1p(test_df['TransactionAmt'])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Email Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### 'P_emaildomain' - 'R_emaildomain'\n",
    "p = 'P_emaildomain'\n",
    "r = 'R_emaildomain'\n",
    "uknown = 'email_not_provided'\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    df[p] = df[p].fillna(uknown)\n",
    "    df[r] = df[r].fillna(uknown)\n",
    "    \n",
    "    # Check if P_emaildomain matches R_emaildomain\n",
    "    df['email_check'] = np.where((df[p]==df[r])&(df[p]!=uknown),1,0)\n",
    "\n",
    "    df[p+'_prefix'] = df[p].apply(lambda x: x.split('.')[0])\n",
    "    df[r+'_prefix'] = df[r].apply(lambda x: x.split('.')[0])\n",
    "\n",
    "## Local test doesn't show any boost here, \n",
    "## but I think it's a good option for model stability \n",
    "\n",
    "## Also, we will do frequency encoding later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device Info Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Device info\n",
    "for df in [train_df, test_df]:\n",
    "    ########################### Device info\n",
    "    df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n",
    "    df['DeviceInfo_device'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "    df['DeviceInfo_version'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
    "    \n",
    "    ########################### Device info 2\n",
    "    df['id_30'] = df['id_30'].fillna('unknown_device').str.lower()\n",
    "    df['id_30_device'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "    df['id_30_version'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
    "    \n",
    "    ########################### Browser\n",
    "    df['id_31'] = df['id_31'].fillna('unknown_device').str.lower()\n",
    "    df['id_31_device'] = df['id_31'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Freq encoding\n",
    "i_cols = ['card1','card2','card3','card5',\n",
    "          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n",
    "          'D1','D2','D3','D4','D5','D6','D7','D8',\n",
    "          'addr1','addr2',\n",
    "          'dist1','dist2',\n",
    "          'P_emaildomain', 'R_emaildomain',\n",
    "          'DeviceInfo','DeviceInfo_device','DeviceInfo_version',\n",
    "          'id_30','id_30_device','id_30_version',\n",
    "          'id_31_device',\n",
    "          'id_33',\n",
    "          'uid','uid2','uid3',\n",
    "         ]\n",
    "\n",
    "for col in i_cols:\n",
    "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts(dropna=False).to_dict()   \n",
    "    train_df[col+'_fq_enc'] = train_df[col].map(fq_encode)\n",
    "    test_df[col+'_fq_enc']  = test_df[col].map(fq_encode)\n",
    "\n",
    "\n",
    "for col in ['DT_M','DT_W','DT_D']:\n",
    "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts().to_dict()\n",
    "            \n",
    "    train_df[col+'_total'] = train_df[col].map(fq_encode)\n",
    "    test_df[col+'_total']  = test_df[col].map(fq_encode)\n",
    "        \n",
    "\n",
    "periods = ['DT_M','DT_W','DT_D']\n",
    "i_cols = ['uid']\n",
    "for period in periods:\n",
    "    for col in i_cols:\n",
    "        new_column = col + '_' + period\n",
    "            \n",
    "        temp_df = pd.concat([train_df[[col,period]], test_df[[col,period]]])\n",
    "        temp_df[new_column] = temp_df[col].astype(str) + '_' + (temp_df[period]).astype(str)\n",
    "        fq_encode = temp_df[new_column].value_counts().to_dict()\n",
    "            \n",
    "        train_df[new_column] = (train_df[col].astype(str) + '_' + train_df[period].astype(str)).map(fq_encode)\n",
    "        test_df[new_column]  = (test_df[col].astype(str) + '_' + test_df[period].astype(str)).map(fq_encode)\n",
    "        \n",
    "        train_df[new_column] /= train_df[period+'_total']\n",
    "        test_df[new_column]  /= test_df[period+'_total']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProductCD\n",
      "card4\n",
      "card6\n",
      "P_emaildomain\n",
      "R_emaildomain\n",
      "M1\n",
      "M2\n",
      "M3\n",
      "M4\n",
      "M5\n",
      "M6\n",
      "M7\n",
      "M8\n",
      "M9\n",
      "id_12\n",
      "id_15\n",
      "id_16\n",
      "id_23\n",
      "id_27\n",
      "id_28\n",
      "id_29\n",
      "id_30\n",
      "id_31\n",
      "id_33\n",
      "id_34\n",
      "id_35\n",
      "id_36\n",
      "id_37\n",
      "id_38\n",
      "DeviceType\n",
      "DeviceInfo\n",
      "uid\n",
      "uid2\n",
      "uid3\n",
      "P_emaildomain_prefix\n",
      "R_emaildomain_prefix\n",
      "DeviceInfo_device\n",
      "DeviceInfo_version\n",
      "id_30_device\n",
      "id_30_version\n",
      "id_31_device\n"
     ]
    }
   ],
   "source": [
    "########################### Encode Str columns\n",
    "# For all such columns (probably not)\n",
    "# we already did frequency encoding (numeric feature)\n",
    "# so we will use astype('category') here\n",
    "for col in list(train_df):\n",
    "    if train_df[col].dtype=='O':\n",
    "        print(col)\n",
    "        train_df[col] = train_df[col].fillna('unseen_before_label')\n",
    "        test_df[col]  = test_df[col].fillna('unseen_before_label')\n",
    "        \n",
    "        train_df[col] = train_df[col].astype(str)\n",
    "        test_df[col] = test_df[col].astype(str)\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train_df[col])+list(test_df[col]))\n",
    "        train_df[col] = le.transform(train_df[col])\n",
    "        test_df[col]  = le.transform(test_df[col])\n",
    "        \n",
    "        train_df[col] = train_df[col].astype('category')\n",
    "        test_df[col] = test_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_parquet('../../data/train_FE001.parquet')\n",
    "test_df.to_parquet('../../data/test_FE001.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['isFraud',\n",
       " 'TransactionDT',\n",
       " 'TransactionAmt',\n",
       " 'ProductCD',\n",
       " 'card1',\n",
       " 'card2',\n",
       " 'card3',\n",
       " 'card4',\n",
       " 'card5',\n",
       " 'card6',\n",
       " 'addr1',\n",
       " 'addr2',\n",
       " 'dist1',\n",
       " 'dist2',\n",
       " 'P_emaildomain',\n",
       " 'R_emaildomain',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C3',\n",
       " 'C4',\n",
       " 'C5',\n",
       " 'C6',\n",
       " 'C7',\n",
       " 'C8',\n",
       " 'C9',\n",
       " 'C10',\n",
       " 'C11',\n",
       " 'C12',\n",
       " 'C13',\n",
       " 'C14',\n",
       " 'D1',\n",
       " 'D2',\n",
       " 'D3',\n",
       " 'D4',\n",
       " 'D5',\n",
       " 'D6',\n",
       " 'D7',\n",
       " 'D8',\n",
       " 'D9',\n",
       " 'D10',\n",
       " 'D11',\n",
       " 'D12',\n",
       " 'D13',\n",
       " 'D14',\n",
       " 'D15',\n",
       " 'M1',\n",
       " 'M2',\n",
       " 'M3',\n",
       " 'M4',\n",
       " 'M5',\n",
       " 'M6',\n",
       " 'M7',\n",
       " 'M8',\n",
       " 'M9',\n",
       " 'V1',\n",
       " 'V2',\n",
       " 'V3',\n",
       " 'V4',\n",
       " 'V5',\n",
       " 'V6',\n",
       " 'V7',\n",
       " 'V8',\n",
       " 'V9',\n",
       " 'V10',\n",
       " 'V11',\n",
       " 'V12',\n",
       " 'V13',\n",
       " 'V14',\n",
       " 'V15',\n",
       " 'V16',\n",
       " 'V17',\n",
       " 'V18',\n",
       " 'V19',\n",
       " 'V20',\n",
       " 'V21',\n",
       " 'V22',\n",
       " 'V23',\n",
       " 'V24',\n",
       " 'V25',\n",
       " 'V26',\n",
       " 'V27',\n",
       " 'V28',\n",
       " 'V29',\n",
       " 'V30',\n",
       " 'V31',\n",
       " 'V32',\n",
       " 'V33',\n",
       " 'V34',\n",
       " 'V35',\n",
       " 'V36',\n",
       " 'V37',\n",
       " 'V38',\n",
       " 'V39',\n",
       " 'V40',\n",
       " 'V41',\n",
       " 'V42',\n",
       " 'V43',\n",
       " 'V44',\n",
       " 'V45',\n",
       " 'V46',\n",
       " 'V47',\n",
       " 'V48',\n",
       " 'V49',\n",
       " 'V50',\n",
       " 'V51',\n",
       " 'V52',\n",
       " 'V53',\n",
       " 'V54',\n",
       " 'V55',\n",
       " 'V56',\n",
       " 'V57',\n",
       " 'V58',\n",
       " 'V59',\n",
       " 'V60',\n",
       " 'V61',\n",
       " 'V62',\n",
       " 'V63',\n",
       " 'V64',\n",
       " 'V65',\n",
       " 'V66',\n",
       " 'V67',\n",
       " 'V68',\n",
       " 'V69',\n",
       " 'V70',\n",
       " 'V71',\n",
       " 'V72',\n",
       " 'V73',\n",
       " 'V74',\n",
       " 'V75',\n",
       " 'V76',\n",
       " 'V77',\n",
       " 'V78',\n",
       " 'V79',\n",
       " 'V80',\n",
       " 'V81',\n",
       " 'V82',\n",
       " 'V83',\n",
       " 'V84',\n",
       " 'V85',\n",
       " 'V86',\n",
       " 'V87',\n",
       " 'V88',\n",
       " 'V89',\n",
       " 'V90',\n",
       " 'V91',\n",
       " 'V92',\n",
       " 'V93',\n",
       " 'V94',\n",
       " 'V95',\n",
       " 'V96',\n",
       " 'V97',\n",
       " 'V98',\n",
       " 'V99',\n",
       " 'V100',\n",
       " 'V101',\n",
       " 'V102',\n",
       " 'V103',\n",
       " 'V104',\n",
       " 'V105',\n",
       " 'V106',\n",
       " 'V107',\n",
       " 'V108',\n",
       " 'V109',\n",
       " 'V110',\n",
       " 'V111',\n",
       " 'V112',\n",
       " 'V113',\n",
       " 'V114',\n",
       " 'V115',\n",
       " 'V116',\n",
       " 'V117',\n",
       " 'V118',\n",
       " 'V119',\n",
       " 'V120',\n",
       " 'V121',\n",
       " 'V122',\n",
       " 'V123',\n",
       " 'V124',\n",
       " 'V125',\n",
       " 'V126',\n",
       " 'V127',\n",
       " 'V128',\n",
       " 'V129',\n",
       " 'V130',\n",
       " 'V131',\n",
       " 'V132',\n",
       " 'V133',\n",
       " 'V134',\n",
       " 'V135',\n",
       " 'V136',\n",
       " 'V137',\n",
       " 'V138',\n",
       " 'V139',\n",
       " 'V140',\n",
       " 'V141',\n",
       " 'V142',\n",
       " 'V143',\n",
       " 'V144',\n",
       " 'V145',\n",
       " 'V146',\n",
       " 'V147',\n",
       " 'V148',\n",
       " 'V149',\n",
       " 'V150',\n",
       " 'V151',\n",
       " 'V152',\n",
       " 'V153',\n",
       " 'V154',\n",
       " 'V155',\n",
       " 'V156',\n",
       " 'V157',\n",
       " 'V158',\n",
       " 'V159',\n",
       " 'V160',\n",
       " 'V161',\n",
       " 'V162',\n",
       " 'V163',\n",
       " 'V164',\n",
       " 'V165',\n",
       " 'V166',\n",
       " 'V167',\n",
       " 'V168',\n",
       " 'V169',\n",
       " 'V170',\n",
       " 'V171',\n",
       " 'V172',\n",
       " 'V173',\n",
       " 'V174',\n",
       " 'V175',\n",
       " 'V176',\n",
       " 'V177',\n",
       " 'V178',\n",
       " 'V179',\n",
       " 'V180',\n",
       " 'V181',\n",
       " 'V182',\n",
       " 'V183',\n",
       " 'V184',\n",
       " 'V185',\n",
       " 'V186',\n",
       " 'V187',\n",
       " 'V188',\n",
       " 'V189',\n",
       " 'V190',\n",
       " 'V191',\n",
       " 'V192',\n",
       " 'V193',\n",
       " 'V194',\n",
       " 'V195',\n",
       " 'V196',\n",
       " 'V197',\n",
       " 'V198',\n",
       " 'V199',\n",
       " 'V200',\n",
       " 'V201',\n",
       " 'V202',\n",
       " 'V203',\n",
       " 'V204',\n",
       " 'V205',\n",
       " 'V206',\n",
       " 'V207',\n",
       " 'V208',\n",
       " 'V209',\n",
       " 'V210',\n",
       " 'V211',\n",
       " 'V212',\n",
       " 'V213',\n",
       " 'V214',\n",
       " 'V215',\n",
       " 'V216',\n",
       " 'V217',\n",
       " 'V218',\n",
       " 'V219',\n",
       " 'V220',\n",
       " 'V221',\n",
       " 'V222',\n",
       " 'V223',\n",
       " 'V224',\n",
       " 'V225',\n",
       " 'V226',\n",
       " 'V227',\n",
       " 'V228',\n",
       " 'V229',\n",
       " 'V230',\n",
       " 'V231',\n",
       " 'V232',\n",
       " 'V233',\n",
       " 'V234',\n",
       " 'V235',\n",
       " 'V236',\n",
       " 'V237',\n",
       " 'V238',\n",
       " 'V239',\n",
       " 'V240',\n",
       " 'V241',\n",
       " 'V242',\n",
       " 'V243',\n",
       " 'V244',\n",
       " 'V245',\n",
       " 'V246',\n",
       " 'V247',\n",
       " 'V248',\n",
       " 'V249',\n",
       " 'V250',\n",
       " 'V251',\n",
       " 'V252',\n",
       " 'V253',\n",
       " 'V254',\n",
       " 'V255',\n",
       " 'V256',\n",
       " 'V257',\n",
       " 'V258',\n",
       " 'V259',\n",
       " 'V260',\n",
       " 'V261',\n",
       " 'V262',\n",
       " 'V263',\n",
       " 'V264',\n",
       " 'V265',\n",
       " 'V266',\n",
       " 'V267',\n",
       " 'V268',\n",
       " 'V269',\n",
       " 'V270',\n",
       " 'V271',\n",
       " 'V272',\n",
       " 'V273',\n",
       " 'V274',\n",
       " 'V275',\n",
       " 'V276',\n",
       " 'V277',\n",
       " 'V278',\n",
       " 'V279',\n",
       " 'V280',\n",
       " 'V281',\n",
       " 'V282',\n",
       " 'V283',\n",
       " 'V284',\n",
       " 'V285',\n",
       " 'V286',\n",
       " 'V287',\n",
       " 'V288',\n",
       " 'V289',\n",
       " 'V290',\n",
       " 'V291',\n",
       " 'V292',\n",
       " 'V293',\n",
       " 'V294',\n",
       " 'V295',\n",
       " 'V296',\n",
       " 'V297',\n",
       " 'V298',\n",
       " 'V299',\n",
       " 'V300',\n",
       " 'V301',\n",
       " 'V302',\n",
       " 'V303',\n",
       " 'V304',\n",
       " 'V305',\n",
       " 'V306',\n",
       " 'V307',\n",
       " 'V308',\n",
       " 'V309',\n",
       " 'V310',\n",
       " 'V311',\n",
       " 'V312',\n",
       " 'V313',\n",
       " 'V314',\n",
       " 'V315',\n",
       " 'V316',\n",
       " 'V317',\n",
       " 'V318',\n",
       " 'V319',\n",
       " 'V320',\n",
       " 'V321',\n",
       " 'V322',\n",
       " 'V323',\n",
       " 'V324',\n",
       " 'V325',\n",
       " 'V326',\n",
       " 'V327',\n",
       " 'V328',\n",
       " 'V329',\n",
       " 'V330',\n",
       " 'V331',\n",
       " 'V332',\n",
       " 'V333',\n",
       " 'V334',\n",
       " 'V335',\n",
       " 'V336',\n",
       " 'V337',\n",
       " 'V338',\n",
       " 'V339',\n",
       " 'id_01',\n",
       " 'id_02',\n",
       " 'id_03',\n",
       " 'id_04',\n",
       " 'id_05',\n",
       " 'id_06',\n",
       " 'id_07',\n",
       " 'id_08',\n",
       " 'id_09',\n",
       " 'id_10',\n",
       " 'id_11',\n",
       " 'id_12',\n",
       " 'id_13',\n",
       " 'id_14',\n",
       " 'id_15',\n",
       " 'id_16',\n",
       " 'id_17',\n",
       " 'id_18',\n",
       " 'id_19',\n",
       " 'id_20',\n",
       " 'id_21',\n",
       " 'id_22',\n",
       " 'id_23',\n",
       " 'id_24',\n",
       " 'id_25',\n",
       " 'id_26',\n",
       " 'id_27',\n",
       " 'id_28',\n",
       " 'id_29',\n",
       " 'id_30',\n",
       " 'id_31',\n",
       " 'id_32',\n",
       " 'id_33',\n",
       " 'id_34',\n",
       " 'id_35',\n",
       " 'id_36',\n",
       " 'id_37',\n",
       " 'id_38',\n",
       " 'DeviceType',\n",
       " 'DeviceInfo',\n",
       " 'DT',\n",
       " 'DT_M',\n",
       " 'DT_W',\n",
       " 'DT_D',\n",
       " 'DT_hour',\n",
       " 'DT_day_week',\n",
       " 'DT_day',\n",
       " 'D9_fixed',\n",
       " 'M_sum',\n",
       " 'M_na',\n",
       " 'ProductCD_target_mean',\n",
       " 'M4_target_mean',\n",
       " 'uid',\n",
       " 'uid2',\n",
       " 'uid3',\n",
       " 'TransactionAmt_check',\n",
       " 'card1_TransactionAmt_mean',\n",
       " 'card1_TransactionAmt_std',\n",
       " 'card2_TransactionAmt_mean',\n",
       " 'card2_TransactionAmt_std',\n",
       " 'card3_TransactionAmt_mean',\n",
       " 'card3_TransactionAmt_std',\n",
       " 'card5_TransactionAmt_mean',\n",
       " 'card5_TransactionAmt_std',\n",
       " 'uid_TransactionAmt_mean',\n",
       " 'uid_TransactionAmt_std',\n",
       " 'uid2_TransactionAmt_mean',\n",
       " 'uid2_TransactionAmt_std',\n",
       " 'uid3_TransactionAmt_mean',\n",
       " 'uid3_TransactionAmt_std',\n",
       " 'TransactionAmt_log1p',\n",
       " 'email_check',\n",
       " 'P_emaildomain_prefix',\n",
       " 'R_emaildomain_prefix',\n",
       " 'DeviceInfo_device',\n",
       " 'DeviceInfo_version',\n",
       " 'id_30_device',\n",
       " 'id_30_version',\n",
       " 'id_31_device',\n",
       " 'card1_fq_enc',\n",
       " 'card2_fq_enc',\n",
       " 'card3_fq_enc',\n",
       " 'card5_fq_enc',\n",
       " 'C1_fq_enc',\n",
       " 'C2_fq_enc',\n",
       " 'C3_fq_enc',\n",
       " 'C4_fq_enc',\n",
       " 'C5_fq_enc',\n",
       " 'C6_fq_enc',\n",
       " 'C7_fq_enc',\n",
       " 'C8_fq_enc',\n",
       " 'C9_fq_enc',\n",
       " 'C10_fq_enc',\n",
       " 'C11_fq_enc',\n",
       " 'C12_fq_enc',\n",
       " 'C13_fq_enc',\n",
       " 'C14_fq_enc',\n",
       " 'D1_fq_enc',\n",
       " 'D2_fq_enc',\n",
       " 'D3_fq_enc',\n",
       " 'D4_fq_enc',\n",
       " 'D5_fq_enc',\n",
       " 'D6_fq_enc',\n",
       " 'D7_fq_enc',\n",
       " 'D8_fq_enc',\n",
       " 'addr1_fq_enc',\n",
       " 'addr2_fq_enc',\n",
       " 'dist1_fq_enc',\n",
       " 'dist2_fq_enc',\n",
       " 'P_emaildomain_fq_enc',\n",
       " 'R_emaildomain_fq_enc',\n",
       " 'DeviceInfo_fq_enc',\n",
       " 'DeviceInfo_device_fq_enc',\n",
       " 'DeviceInfo_version_fq_enc',\n",
       " 'id_30_fq_enc',\n",
       " 'id_30_device_fq_enc',\n",
       " 'id_30_version_fq_enc',\n",
       " 'id_31_device_fq_enc',\n",
       " 'id_33_fq_enc',\n",
       " 'uid_fq_enc',\n",
       " 'uid2_fq_enc',\n",
       " 'uid3_fq_enc',\n",
       " 'DT_M_total',\n",
       " 'DT_W_total',\n",
       " 'DT_D_total',\n",
       " 'uid_DT_M',\n",
       " 'uid_DT_W',\n",
       " 'uid_DT_D']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in train_df.columns]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
